import numpy as np
import matplotlib.pyplot as plt
import random


input_matrix = np.genfromtxt("normalized_data.txt")
hidden_layer_neuron_count = 1000
batch_size = 160
mu = .01
# letter_distribution is the number of each letter that are in the data set, used for making sure there is equal
# representation for each letter, as the matrix is sorted by letter, the order is 1-26 = a-z
# note: -1 is put in the 0 slot so it is a straight 1-26 a-z instead of 0-25, since the numbers saved in the text
# file is numbered 1-26, -1 will throw an error, so it should never be used
letter_distribution = [-1, 789, 766, 736, 805, 768, 775, 773, 734, 755, 747, 739, 761, 792, 783, 753, 803, 783, 758,
                       748, 796, 813, 764, 752, 787, 786, 734]

# creating the layers for the ANN, there are 16 inputs for the hidden layer, and each neuron in the hidden layer
# is an input for the output layer, so those to numbers are the same
W1 = np.random.random_integers(-1, 1, (16, hidden_layer_neuron_count))
W2 = np.random.random_integers(-1, 1, (hidden_layer_neuron_count, 26))
heat_map = np.zeros((26, 26))


def sigmoid(number):
    return 1/(1 + np.exp(-number))


def sigmoid_prime(number):
    return np.exp(-number)/((1+np.exp(-number))**2)


def soft_max(array):
    Z = np.sum(np.exp(array))
    return np.exp(array) / Z


def gradient_descent(output_array, sigma_array):
    operand_1 = []
    for thing in output_array:
        temp = np.full(26, 0.1)
        temp[int(thing[0])-1] = .9
        operand_1.append(temp)
    return sigma_array - operand_1


# creating the training and validation set, which are 80/20 for each letter, creating a 80/20 of the entire set that is
# representative. I needed to convert the numpy arrays to lists so i could use the method I used to create
# the validation set, I then make them a np array when I finish, distribution is made for the new sets, so batches can
# be equal representation as well
previous_index = 0
training_set = []
validation_set = []
test = []
training_distribution = []
for item in input_matrix:
    test.append(list(item))
for instances in letter_distribution[1:]:
    train_temp = random.sample(test[previous_index:previous_index + instances], int(instances * .8))
    valid_temp = [item for item in test[previous_index:previous_index + instances] if item not in train_temp]
    # create distribution of letters
    training_distribution.append(len(train_temp))
    # add the sets for each letter, remaking them as np arrays
    training_set.extend(np.array(train_temp))
    validation_set.extend(np.array(valid_temp))
    # update the previous index so it will go to the next letter
    previous_index = previous_index + instances
# remake the sets as np arrays
training_set = np.array(training_set)
validation_set = np.array(validation_set)


training_accuracy = []
valid_accuracy = []
epochs = []
epoch = 0

for not_used in range(50000):
    if not_used % int(16000/batch_size) == 0:
        epochs.append(epoch)
        print("Epoch: " + str(epoch))
        epoch += 1
    # Creating batches for training
    X = []
    prev_index = 0
    for instances in training_distribution:
        batch_temp = random.sample(list(training_set[prev_index:prev_index + instances]), int(round(batch_size/26)))
        X.extend(np.array(batch_temp))
        prev_index = prev_index + instances
    X = np.array(X)
    # getting the matrices for the hidden layer
    S_1 = np.dot(X[:, 1:], W1)
    Z_1 = sigmoid(S_1)
    Z_prime_1 = sigmoid_prime(S_1)

    # getting the matrices for the output layer
    S_2 = np.dot(Z_1, W2)
    Z_2 = sigmoid(S_2)
    Z_prime_2 = sigmoid_prime(S_2)

    # getting the accuracy and error
    output = []
    correct_count = 0
    for item in Z_2:
        output.append(soft_max(item))
    for index in range(len(output)):
        if (np.argmax(output[index])+1) == X[index, 0]:
            correct_count += 1
    if not_used % int(16000/batch_size) == 0:
        training_accuracy.append(correct_count / X[:, 0].size)

    delta3 = np.transpose(gradient_descent(X, Z_2))
    delta2 = np.dot(W2, delta3)
    delta2 = delta2 * np.transpose(Z_prime_1)
    D_W_2 = -mu * np.transpose(np.dot(delta3, Z_1))
    D_W_1 = -mu * np.transpose(np.dot(delta2, X[:, 1:]))
    W2 = W2 + D_W_2
    W1 = W1 + D_W_1

    #  Running validations on epoch completion
    if not_used % int(16000 / batch_size) == 0:
        v_S_1 = np.dot(validation_set[:, 1:], W1)
        v_Z_1 = sigmoid(v_S_1)
        v_S_2 = np.dot(v_Z_1, W2)
        v_Z_2 = sigmoid(v_S_2)
        valid_correct_count = 0
        v_output = []
        for item in v_Z_2:
            v_output.append(soft_max(item))
        for index in range(len(v_output)):
            if (np.argmax(v_output[index])+1) == validation_set[index, 0]:
                heat_map[int(np.argmax(v_output[index])), int(validation_set[index, 0] - 1)] += 1
                valid_correct_count += 1
        print("Validation Accuracy: " + str(valid_correct_count / validation_set[:, 0].size * 100) + "%")
        valid_accuracy.append(valid_correct_count / validation_set[:, 0].size)
plt.figure(1)
plt.plot(epochs, training_accuracy, color="yellow")
plt.plot(epochs, valid_accuracy, color="green")
plt.hlines(.85, 0, len(epochs), color="red", label="85% Threshold")
plt.ylabel("Accuracy")
plt.xlabel("Epoch")
plt.title("Accuracy of the ANN\nTraining in yellow, Validation in green\n"
          "85% Threshold in red")

plt.figure(2)
plt.imshow(heat_map, cmap='gray', interpolation='nearest')
plt.show()
